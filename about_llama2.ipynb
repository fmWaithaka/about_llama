{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPgmwa5zTFYf8jYQLA0rzne",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/waithakaFM910/about_llama/blob/main/about_llama2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Llama 2 Model  🦙\n",
        "\n",
        "![picture](https://eu-images.contentstack.com/v3/assets/blt6b0f74e5591baa03/blt98d8a946b63c9b5f/64b7170ab314c94aa481d8c3/Untitled_design_(1).jpg?width=850&auto=webp&quality=95&format=jpg&disable=upscale)\n",
        "\n",
        "\n",
        "**About Llama 2**\n",
        "* Llama 2 from metaAI is the next generation of our *open source large language model* and is available for free for research and commercial use.\n",
        "* It is a collection of pretrained and fine tuned large language model ranging from 7 Billion to 70 Billion parameters.\n",
        "* *Llama 2 chat* are optimized for dialoge use cases.\n",
        "* Llama 2 chat chat has a safety and bias mitigation\n",
        "* Llama 2 supports longer context lengths, up to 4096 tokens.\n",
        "* Read more on Llama chat on their [blog](https://ai.meta.com/resources/models-and-libraries/llama/)\n",
        "* There is also llama 2 model called *Code Llama* which is a state-oof-art LLM capable od generating code, and natural language about code, from both code and natural language prompts.\n",
        "* *Code Llama* is also free for research and commercial use.\n",
        "* Check out [Code Llama blog](https://ai.meta.com/blog/code-llama-large-language-model-coding/) for more."
      ],
      "metadata": {
        "id": "FH5SA5pyt28r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Why Llama 2 🦙?\n",
        "\n",
        "Llama 2 is still new and moreso it is *open source* and free for both research and commercial use.\n",
        "It has also outperformed other open source models which make it play in the big league of AI models such as ChatGPT.\n",
        "\n",
        "So, if you're looking for an AI model that's both fun and fierce, Llama 2 might just be your new best buddy in the world of AI. Give it a whirl, and let the llama magic begin! 🌟\n",
        "\n",
        "\n",
        "Some Benefits that comes along by making Llama 2 open source and free:\n",
        "  1. **Data Privacy** - Llama can be downloaded and install run on a local machine or a server. This gives more control over your data and reduce the risk of exposure to a 3rd party.\n",
        "  2. **Customization** - allows devs to train LLMs with their own data and some filtering on some topics if they want to apply.\n",
        "  3. **Affordable**- it lets you train sophisticated LLMs without worrying of expensive hardware.\n",
        "  4. **Democratizing AI** - it is open room for further research which can be used for solving real-world problem\n",
        "\n"
      ],
      "metadata": {
        "id": "kkj0FfkNub7t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## How to run and use Llama 2\n",
        "\n",
        "You can run Llama 2 model using various methods.\n",
        "  > * **Local:** You can download and run Llama 2 locally on your own machine. This requires expensive hardware such as GPUs, as well as the time and expertise to set up and configure the model.\n",
        "* **Cloud:** You can use a cloud computing platform such as Google Cloud Platform, Amazon Web Services, or Microsoft Azure to run Llama 2. This can be a good option if you do not have the hardware or expertise to run Llama 2 locally.\n",
        "* **API:** You can use an API such as the Replicate API to run Llama 2 without having to install or configure any software on your own machine. This can be a good option if you want to use Llama 2 in a production environment or if you do not have the technical expertise to run Llama 2 locally or on a cloud computing platform.\n",
        "\n",
        "In this blog we are going to see how to run Llama 2 using *Replicate API*\n"
      ],
      "metadata": {
        "id": "WwymlhaZ33U_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What is Replicate API?\n",
        "Replicate allows you rum machine learning model easly without the need to understand how machine learning works.\n",
        "It handles complexities of managing the infrastruture and dependancies needed to run machine learning models so that developers can focus on building their application.\n",
        "> It is worth noting the Replicate tokens comes at a fee but you can get free token on your first account you create on their [website](https://replicate.com/)"
      ],
      "metadata": {
        "id": "N8ZaYII191YY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Let's get started with running Llama using Replicate"
      ],
      "metadata": {
        "id": "J-plrrsF_mMC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "First we install the library"
      ],
      "metadata": {
        "id": "0_gN4Du__00l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install replicate\n",
        "from google.colab import output\n",
        "output.clear()"
      ],
      "metadata": {
        "id": "9yWGiWp7ACPu"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's the and Replicate API token. You can get your own token from [this link](https://replicate.com/account)"
      ],
      "metadata": {
        "id": "MUZon4uQAKVN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import replicate\n",
        "from getpass import getpass\n",
        "\n",
        "REPLICATE_API_TOKEN = getpass()\n",
        "os.environ[\"REPLICATE_API_TOKEN\"]  = REPLICATE_API_TOKEN\n",
        "\n",
        "## define some helper variables\n",
        "llama2_70b = \"replicate/llama-2-70b-chat:2c1608e18606fad2812020dc541930f2d0495ce32eee50074220b87300bc16e1\"\n",
        "llama2_13b = \"a16z-infra/llama-2-13b-chat:2a7f981751ec7fdf87b5b91ad4db53683a98082e9ff7bfd12c8cd5ea85980a52\"\n",
        "llama2_7b = \"a16z-infra/llama7b-v2-chat:4f0a4744c7295c024a1de15e1a63c880d3da035fa1f49bfd344fe076074c8eea\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZluvznkrAlYe",
        "outputId": "877717e9-b789-4adf-8781-faf5390792f7"
      },
      "execution_count": 2,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "··········\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's now run our model and see the results"
      ],
      "metadata": {
        "id": "F7NQGfjFEHeH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "output = replicate.run(\n",
        "    llama2_13b,\n",
        "    input={\n",
        "        \"prompt\": \"Explain natural language processing and Large Language Model?\",\n",
        "         \"temperature\":1.0, \"top_p\":0.9, \"max_length\":128, \"repetition_penalty\":1\n",
        "    }\n",
        ")\n",
        "''.join(output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 244
        },
        "id": "ah5saBa-F60a",
        "outputId": "e99976ee-e2ff-4138-9d4b-4f8fc09ac5d7"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\" Hello! I'd be happy to help you understand natural language processing (NLP) and large language models (LLMs).\\n\\nNatural language processing is a subfield of artificial intelligence (AI) that focuses on the interaction between computers and human language. The goal of NLP is to enable computers to understand, interpret, and generate human language in a way that is both accurate and contextually appropriate.\\n\\nLarge language models, on the other hand, are a type of AI model that is specifically designed to process and generate human language. These models are trained on vast amounts of text data, and they use this training to learn patterns and relationships within the language.\\n\\nOne of the key features of LLMs is their ability to generate text that is coherent and contextually appropriate. This is achieved through the use of neural networks, which are a type of machine learning algorithm that is particularly well-suited to processing and generating natural language.\\n\\nThere are many different types of LLMs, each with their own strengths and weaknesses. Some of the most popular types of LLMs include:\\n\\n1. Recurrent neural networks (RNNs): These models are particularly good at processing sequential data, such as text. They use a feedback loop to maintain a hidden state that captures information from previous inputs, allowing them to generate text that is contextually appropriate.\\n2. Transformers: These models are based on attention mechanisms that allow them to focus on specific parts of the input when generating output. This makes them particularly well-suited to tasks that require the model to understand the relationships between different parts of the input.\\n3. Generative adversarial networks (GANs): These models consist of two neural networks that work together to generate text. One network generates text, while the other network evaluates the generated text and provides feedback to the first network. This feedback loop allows the model to improve over time, generating more coherent and contextually appropriate text.\\n\\nLLMs have many potential applications, such as:\\n\\n1. Language translation: LLMs can be trained on large amounts of text data in multiple languages, allowing them to learn the relationships between languages and generate translations that are both accurate and contextually appropriate.\\n2. Text summarization: LLMs can be used to summarize long documents, extracting the most important information\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Explaining the parameters\n",
        "1. `Temperature:` Temperature is a hyperparameter that controls the randomness of the model's output. A higher temperature, such as 1.0, makes the output more random and creative, while a lower temperature, such as 0.2, makes the output more focused and deterministic. It affects the diversity of the generated text. In your code, it appears to be a variable representing the temperature setting for the generation process.\n",
        "\n",
        "2. `Top-p (Top Probability):` Top-p is another hyperparameter that controls the diversity of the output. It determines the probability mass to consider for each token in the model's vocabulary. A higher top_p value (e.g., 0.9) allows a larger portion of the vocabulary to be considered, potentially leading to more diverse output. A lower value (e.g., 0.2) limits the vocabulary considered, resulting in more focused responses.\n",
        "\n",
        "3. `Max Length:` max_length specifies the maximum length (in tokens) for the generated output. It can be used to limit the length of the response to a certain number of tokens. This is useful to prevent excessively long responses that might not be desirable.\n",
        "\n",
        "4. `Repetition Penalty:` The repetition penalty is a value that discourages the model from repeating the same phrases or patterns in its output. A higher repetition penalty, such as 2.0, strongly discourages repetition, while a lower value, such as 1.0, allows more repetition in the generated text. It helps control the fluency and coherence of the generated responses."
      ],
      "metadata": {
        "id": "RvPvK8zfHBHS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Let's do a bit of prompt engineering\n",
        "Prompt engineering is the process of designing prompts that elicit desired responses from a chatbot.\n",
        "\n",
        "Let’s say you wanted to write a chatbot that talks like it is talking to a teenager. One way to do this would be to prepend “you are responding to a teenager” to every prompt.\n",
        "\n",
        "This gets tedious after a while. Instead, we can set a system_prompt ”You are a pirate,” and the model will understand your request without having to be told in every prompt:\n",
        "\n",
        "For prompt engineering we are going to use `system_prompt`\n",
        "\n",
        "> 💡 A `system_prompt` is text that is prepended to the prompt. It’s used in a chat context to help guide or constrain model behavior."
      ],
      "metadata": {
        "id": "iRntMAUrHNhT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "output = replicate.run(\n",
        "    llama2_70b,\n",
        "    input={\n",
        "        \"prompt\": \"Explain natural language processing and Large Language Model?\",\n",
        "        \"system_prompt\": \"You are responding to a teenager\"\n",
        "    }\n",
        ")\n",
        "\n",
        "''.join(output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 244
        },
        "id": "dRJfBs8pNtu8",
        "outputId": "f55b2a7d-efc3-4e7f-b406-4d0c3d92b0f9"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' Hey there! I\\'d be happy to explain natural language processing and large language models in a way that\\'s easy to understand.\\n\\nNatural language processing (NLP) is a field of study that focuses on how computers can understand, interpret, and generate human language. It\\'s like teaching a computer to understand and speak human languages, just like we do. NLP is used in things like chatbots, virtual assistants, and language translation apps.\\n\\nA large language model (LLM) is a type of artificial intelligence that can process and analyze huge amounts of text data. Think of it like a super-smart computer program that can read and understand a lot of text all at once. LLMs are trained on vast amounts of text data, like books, articles, and websites, which helps them learn patterns and relationships in language. This training allows them to perform tasks like generating text, summarizing content, and answering questions.\\n\\nOne really cool thing about LLMs is their ability to generate text that looks and sounds like it was written by a human. They can even create new sentences or paragraphs that are similar to the ones they were trained on. This is because they\\'ve learned the patterns and structures of language, so they can create new examples that are similar but not exactly the same as what they\\'ve seen before.\\n\\nFor example, imagine you gave an LLM a bunch of news articles about sports. After analyzing the articles, the LLM could generate a new article about a sports game that hasn\\'t happened yet, using vocabulary and sentence structures it\\'s learned from the training data. It might look something like this: \"In tonight\\'s matchup between the Red Sox and the Yankees, both teams are looking strong. The Red Sox have been performing well this season, with a 10-game winning streak. But the Yankees have been struggling with injuries and tough competition. Despite these challenges, the Yankees have shown grit and determination, and tonight\\'s game promises to be an exciting one.\"\\n\\nAs you can see, the LLM has generated a believable and coherent paragraph about a fictional sports game, using vocabulary and sentence structures it\\'s learned from its training data. That\\'s pretty impressive!\\n\\nI hope'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rAzg67fkPQ3F"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}